# Main entrypoint of the workflow.
# Please follow the best practices:
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there.

# Possible values are 'NA' (North Atlantic), 'SA' (South Atlantic), 'EP' (Eastern North Pacific, which includes                                                the Central Pacific region), 'WP' (Western North Pacific), 'SP' (South Pacific), 'SI' (South Indian), 'NI' (North Indian).

"""This pipeline generates tropical cyclone impacts and associate coastal surge impacts data
using the Climada modeling plateform based on IBTrACS historical tropical cyclone tracks.

End results are at the global scope for both type of impacts, for both historical period and
choosen RCPs, and future years.

It uses Climada's updated synthetic track generation to simulate a choosen number of tracks `nsynth`
from historical ones with a random-walk process. Then, it computes maximum sustained wind-speed using the
Holland 2008 model. Default resolution is 0.041666659999975764 degrees (i.e. 150 arcsec).

Effects of climate change are simulated using so-called Knutson factors which scale the intensity and
frequency of events to a given RCP and future year.

Coastal surges impacts are estimated using a simple bathtub model and elevation data and via down-scaling
the wind field at a 0.0
"""

years = list(range(config["start"], config["end"], 1))

rule create_centroids:
    output:
        "global_centroids.hdf5",
    params:
        centroid_resolution=config["centroid_resolution"]
    log:
        "logs/create_centroids.log",
    resources:
        mem_mb_per_cpu=12000,
    conda:
        "climada_env_TC"
    script:
        "scripts/create_centroids.py"


rule tracks_basin_year:
    output:
        "tracks/{basin}/IBTracs_{basin}_{year}.hdf5",
    params:
        timestep=1,
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=8000,
        runtime=240,
    log:
        "logs/find_tracks_{basin}_{year}.log",
    script:
        "scripts/find_tracks_basin_year.py"


def synth_track_memb(wildcards, attempt):
    return 1000 * (config["nsynth"] // 2) * attempt


rule synth_tracks_basin_year:
    input:
        "tracks/{basin}/IBTracs_{basin}_{year}.hdf5",
    output:
        "tracks/{basin}/IBTracs_synth_{basin}_{year}.hdf5",
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
    params:
        nsynth=config["nsynth"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=synth_track_memb,
        runtime=240,
    log:
        "logs/generate_synths_tracks_{basin}_{year}.log",
    script:
        "scripts/generate_synths_tracks_basin_year.py"


checkpoint tcs_split_basin_year_tracks:
    input:
        tracks="tracks/{basin}/IBTracs_synth_{basin}_{year}.hdf5",
    output:
        directory("tracks/{basin}/{year}/synth_splits/"),
    params:
        max_tracks=config["max_tracks_per_split"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=8000,
        runtime=240,
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
    log:
        "logs/split_synth_tracks_{basin}_{year}.log",
    script:
        "scripts/split_synth_tracks.py"


rule tcs_basin_year_split:
    input:
        tracks="tracks/{basin}/{year}/synth_splits/IBTracs_synth_{basin}_{year}_split_{i}.hdf5",
        centroids="global_centroids.hdf5",
    output:
        "tropcyc/{basin}/TCs_{basin}_{year}_split_{i}.hdf5",
    params:
        buf=5,
        max_memory_gb=14,
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
        i="\d+"
    log:
        "logs/generate_TCs_{basin}_{year}_split_{i}.log",
    script:
        "scripts/generate_TCs_basin_year.py"


rule surges_basin_year_split:
    input:
        tropcyc="tropcyc/{basin}/TCs_{basin}_{year}_split_{i}.hdf5",
        slr=expand(
            "{slr_data}/{{ssp}}/total_{{ssp}}_medium_confidence_values.nc",
            slr_data=config["slr_data_path"],
        ),
        dem=config["dem_topo_path"],
    output:
        "surge/{basin}/TCSurges_{ssp}_{slr_year}slr_{basin}_{year}_split_{i}.hdf5",
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
        i="\d+",
        ssp="ssp119|ssp126|ssp245|ssp370|ssp585",
        slr_year="20[2-9]0|21[0-5]0",
    params:
        higher_res=config["higher_res_surge"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    log:
        "logs/generate_TCs_{ssp}_{slr_year}slr_{basin}_{year}_{i}.log",
    script:
        "scripts/generate_TCs_surges_basin_year.py"


def aggregate_tcs(wildcards):
    checkpoint_output = checkpoints.tcs_split_basin_year_tracks.get(**wildcards).output[
        0
    ]
    return expand(
        "tropcyc/{{basin}}/TCs_{{basin}}_{{year}}_split_{i}.hdf5",
        i=glob_wildcards(
            os.path.join(
                checkpoint_output, "IBTracs_synth_{basin}_{year}_split_{i}.hdf5"
            )
        ).i,
    )


def aggregate_surges(wildcards):
    checkpoint_output = checkpoints.tcs_split_basin_year_tracks.get(**wildcards).output[
        0
    ]
    return expand(
        "surge/{{basin}}/TCSurges_{{ssp}}_{{slr_year}}slr_{{basin}}_{{year}}_split_{i}.hdf5",
        i=glob_wildcards(
            os.path.join(
                os.getcwd(),
                checkpoint_output, "IBTracs_synth_{basin}_{year}_split_{i}.hdf5"
            )
        ).i,
    )


rule gather_split_tcs:
    input:
        aggregate_tcs,
    output:
        "tropcyc/{basin}/TCs_{basin}_{year}.hdf5",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
    log:
        "logs/gather_split_TCs_{basin}_{year}.log",
    script:
        "scripts/concatenate_all_basins_tcs.py"


rule gather_split_surges:
    input:
        aggregate_surges,
    output:
        "surge/{basin}/TCSurges_{ssp}_{slr_year}slr_{basin}_{year}.hdf5",
    conda:
        "climada_env_TC"
    wildcard_constraints:
        basin="[A-Z]+",
        year="[0-9]{4}",
        ssp="ssp119|ssp126|ssp245|ssp370|ssp585",
        slr_year="20[2-9]0|21[0-5]0",
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    log:
        "logs/gather_split_TCSurges_{ssp}_{slr_year}slr_{basin}_{year}.log",
    script:
        "scripts/gather_split_surges.py"


rule concatenate_all_years_tcs:
    input:
        expand("tropcyc/{{basin}}/TCs_{{basin}}_{year}.hdf5", year=years),
    output:
        expand(
            "tropcyc/{{basin}}/TCs_{{basin}}_{start}_{end}.hdf5",
            start=config["start"],
            end=config["end"],
        ),
    wildcard_constraints:
        basin="[A-Z]+",
    resources:
        mem_mb_per_cpu=24000,
    conda:
        "climada_env_TC"
    log:
        "logs/concatenate_all_years_tcs_{basin}.log",
    script:
        "scripts/concatenate_all_years_tcs.py"


rule concatenate_all_basins_tcs:
    input:
        expand("tropcyc/{basin}/TCs_{basin}_1980_2023.hdf5", basin=basins),
    output:
        "tropcyc/all_basins_1980_2023.hdf5",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=24000,
    log:
        "logs/concatenate_all_basins_tcs.log",
    script:
        "scripts/concatenate_all_basins_tcs.py"
