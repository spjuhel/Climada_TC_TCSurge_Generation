# Main entrypoint of the workflow.
# Please follow the best practices:
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there.

# Possible values are 'NA' (North Atlantic), 'SA' (South Atlantic), 'EP' (Eastern North Pacific, which includes                                                the Central Pacific region), 'WP' (Western North Pacific), 'SP' (South Pacific), 'SI' (South Indian), 'NI' (North Indian).

"""This pipeline generates tropical cyclone impacts and associate coastal surge impacts data
using the Climada modeling plateform based on IBTrACS historical tropical cyclone tracks.

End results are at the global scope for both type of impacts, for both historical period and
choosen RCPs, and future years.

It uses Climada's updated synthetic track generation to simulate a choosen number of tracks `nsynth`
from historical ones with a random-walk process. Then, it computes maximum sustained wind-speed using the
Holland 2008 model. Default resolution is 0.041666659999975764 degrees (i.e. 150 arcsec).

Effects of climate change are simulated using so-called Knutson factors which scale the intensity and
frequency of events to a given RCP and future year.

Coastal surges impacts are estimated using a simple bathtub model and elevation data and via down-scaling
the wind field at a 0.008 degrees (~29 arcsec).

Copyright (C) 2024  Samuel Juhel
"""

tracks_year_range = list(range(config["start"], config["end"]+1, 1))


wildcard_constraints:
    genesis_basin=r"[A-Z]+",
    tracks_year=r"[0-9]{4}",
    ssp=r"nossp|ssp119|ssp126|ssp245|ssp370|ssp585",
    slr_year=r"no|20[2-9]0|21[0-5]0",
    split=r"\d+",
    climate_scenario=r"historical|(rcp26|rpc45|rcp60|rcp85)_(2100|20\d\d)",


rule create_centroids:
    """Creates global centroids grid at `centroid_resolution` resolution"""
    output:
        "global_centroids.hdf5",
    params:
        centroid_resolution=config["centroid_resolution"],
    log:
        "logs/create_centroids.log",
    resources:
        mem_mb_per_cpu=12000,
    conda:
        "climada_env_TC"
    script:
        "scripts/create_centroids.py"


rule tracks_basin_year:
    """Generate file with all tracks originating in `genesis_basin` for year `tracks_year`"""
    output:
        "tracks/{genesis_basin}/IBTracs_{genesis_basin}_{tracks_year}.hdf5",
    params:
        timestep=1,
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=8000,
        runtime=240,
    log:
        "logs/find_tracks_{genesis_basin}_{tracks_year}.log",
    script:
        "scripts/find_tracks_basin_year.py"


def synth_track_memb(wildcards, attempt):
    return 1000 * (config["nsynth"] // 2) * attempt


rule synth_tracks_basin_year:
    """Generate file with `nsynth` synthetic tracks from historical ones in `genesis_basin` for year `tracks_year`"""
    input:
        "tracks/{genesis_basin}/IBTracs_{genesis_basin}_{tracks_year}.hdf5",
    output:
        "tracks/{genesis_basin}/IBTracs_synth_{genesis_basin}_{tracks_year}.hdf5",
    params:
        nsynth=config["nsynth"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=synth_track_memb,
        runtime=240,
    log:
        "logs/generate_synths_tracks_{genesis_basin}_{tracks_year}.log",
    script:
        "scripts/generate_synths_tracks_basin_year.py"


checkpoint split_basin_year_tracks:
    """Split synthetic tracks of a particular basin, year into files with at most `max_tracks_per_split` tracks"""
    input:
        tracks="tracks/{genesis_basin}/IBTracs_synth_{genesis_basin}_{tracks_year}.hdf5",
    output:
        directory("tracks/{genesis_basin}/{tracks_year}/synth_splits/"),
    params:
        max_tracks=config["max_tracks_per_split"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=8000,
        runtime=240,
    log:
        "logs/split_synth_tracks_{genesis_basin}_{tracks_year}.log",
    script:
        "scripts/split_synth_tracks.py"

def get_mb_for_split(wildcards, attempt):
    return max((attempt-1) * 64000,20000)

rule tcs_basin_year_split:
    """Generate tropical cyclones from synthetic tracks (for a split file)"""
    input:
        tracks="tracks/{genesis_basin}/{tracks_year}/synth_splits/IBTracs_synth_{genesis_basin}_{tracks_year}_split_{split}.hdf5",
        centroids="global_centroids.hdf5",
    output:
        "tropcyc/{genesis_basin}/historical/TCs_{genesis_basin}_{tracks_year}_historical_split_{split}.hdf5",
    params:
        buf=5,
        max_memory_gb=14,
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=get_mb_for_split,
        runtime=480,
    log:
        "logs/generate_TCs_{genesis_basin}_{tracks_year}_split_{split}.log",
    benchmark:
        "benchmark/generate_TCs_{genesis_basin}_{tracks_year}_split_{split}.tsv",
    script:
        "scripts/generate_TCs_basin_year.py"


def aggregate_tcs(wildcards):
    checkpoint_output = checkpoints.split_basin_year_tracks.get(**wildcards).output[0]
    return expand(
        "tropcyc/{{genesis_basin}}/historical/TCs_{{genesis_basin}}_{{tracks_year}}_historical_split_{split}.hdf5",
        split=glob_wildcards(
            os.path.join(
                checkpoint_output,
                "IBTracs_synth_{genesis_basin}_{tracks_year}_split_{split}.hdf5",
            )
        ).split,
    )


rule gather_split_tcs:
    input:
        aggregate_tcs,
    output:
        "tropcyc/{genesis_basin}/historical/TCs_{genesis_basin}_{tracks_year}_historical.hdf5",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    log:
        "logs/gather_split_TCs_{genesis_basin}_{tracks_year}_historical.log",
    script:
        "scripts/concatenate_all_basins_tcs.py"


rule concatenate_all_years_tcs:
    input:
        expand(
            "tropcyc/{{genesis_basin}}/historical/TCs_{{genesis_basin}}_{tracks_year}_historical.hdf5",
            tracks_year=tracks_year_range,
        ),
    output:
        "tropcyc/{genesis_basin}/historical/TCs_{genesis_basin}_all_historical.hdf5",
    resources:
        mem_mb_per_cpu=24000,
    conda:
        "climada_env_TC"
    log:
        "logs/concatenate_all_years_tcs_{genesis_basin}_historical.log",
    script:
        "scripts/concatenate_all_years_tcs.py"


rule concatenate_all_basins_tcs:
    input:
        expand(
            "tropcyc/{genesis_basin}/{{climate_scenario}}/TCs_{genesis_basin}_all_{{climate_scenario}}.hdf5",
            genesis_basin=config["genesis_basins"],
        ),
    output:
        "tropcyc/{climate_scenario}/TCs_all_basins_{climate_scenario}.hdf5",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=24000,
    log:
        "logs/concatenate_all_basins_tcs_{climate_scenario}.log",
    script:
        "scripts/concatenate_all_basins_tcs.py"


rule all_historical_tcs:
    input:
        "tropcyc/historical/TCs_all_basins_historical.hdf5"

rule apply_climate_change:
    input:
        tropcyc="tropcyc/historical/TCs_all_basins_historical.hdf5",
    output:
        expand(
            "tropcyc/{climate_scenario}/TCs_all_basins_{climate_scenario}.hdf5",
            climate_scenario=[c for c in config["climate_scenarios"] if c!="historical"],
        ),
    resources:
        mem_mb_per_cpu=24000,
    conda:
        "climada_env_TC"
    log:
        "logs/apply_climate_change.log",
    script:
        "scripts/apply_climate_change.py"


checkpoint split_basin_year_TCs:
    """Split TCs into files with at most `max_tcs_per_split` tracks"""
    input:
        tropcyc="tropcyc/{climate_scenario}/TCs_all_basins_{climate_scenario}.hdf5",
    output:
        directory("tropcyc/{climate_scenario}/splits/"),
    params:
        max_tcs=config["max_tcs_per_split"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=8000,
        runtime=240,
    log:
        "logs/split_TCs_{climate_scenario}.log",
    script:
        "scripts/split_TCs.py"


rule surges_basin_year_split:
    input:
        tropcyc="tropcyc/{climate_scenario}/splits/TCs_{climate_scenario}_split_{split}.hdf5",
        slr=expand(
            "{slr_data}/{{ssp}}/total_{{ssp}}_medium_confidence_values.nc",
            slr_data=config["slr_data_path"],
        ),
        dem=config["dem_topo_path"],
    output:
        "surge/{climate_scenario}/splits/TCSurges_{ssp}_{slr_year}slr_{climate_scenario}_split_{split}.hdf5",
    params:
        higher_res=config["higher_res_surge"],
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    log:
        "logs/generate_TCs_{ssp}_{slr_year}slr_{climate_scenario}_{split}.log",
    benchmark:
        "benchmark/generate_TCs_{ssp}_{slr_year}slr_{climate_scenario}_{split}.tsv",
    script:
        "scripts/generate_TCs_surges_basin_year.py"


def aggregate_surges(wildcards):
    checkpoint_output = checkpoints.split_basin_year_TCs.get(**wildcards).output[0]
    return expand(
        "surge/{{climate_scenario}}/splits/TCSurges_{{ssp}}_{{slr_year}}slr_{{climate_scenario}}_split_{split}.hdf5",
        split=glob_wildcards(
            os.path.join(
                os.getcwd(),
                checkpoint_output,
                "TCs_{climate_scenario}_split_{split}.hdf5",
            )
        ).split,
    )


rule gather_split_surges:
    input:
        aggregate_surges,
    output:
        "surge/{climate_scenario}/TCSurges_{ssp}_{slr_year}slr_all_{climate_scenario}.hdf5",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=16000,
        runtime=240,
    log:
        "logs/gather_split_TCSurges_{ssp}_{slr_year}slr_{climate_scenario}.log",
    script:
        "scripts/gather_split_surges.py"


rule all_tcs:
    input:
        expand(
            "tropcyc/{climate_scenario}/TCs_all_basins_{climate_scenario}.hdf5",
            climate_scenario=config["climate_scenarios"],
        ),


rule all_surges:
    input:
        expand(
            "surge/{climate_scenario}/surge_all_basins_{ssp}_{slr_year}slr_{climate_scenario}.hdf5",
            climate_scenario=[c for c in config["climate_scenarios"] if c!="historical"],
            ssp=config["slr_ssp"],
            slr_year=config["slr_years"],
        ),
        "surge/historical/surge_all_basins_nossp_noslr_historical.hdf5"


rule tcs_per_countries:
    input:
        global_haz="tropcyc/{climate_scenario}/TCs_all_basins_{climate_scenario}.hdf5"
    output:
        expand("tropcyc/{{climate_scenario}}/tropcyc_{country}_{{climate_scenario}}.hdf5", country=config["countries"])
    params:
        countries = config["countries"]
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=24000,
    log:
        "logs/get_countries_tcs_{climate_scenario}.log",
    script:
        "scripts/get_country_tc.py"

rule all_tcs_countries:
    input:
        expand("tropcyc/{climate_scenario}/tropcyc_{country}_{climate_scenario}.hdf5",
               country=config["countries"],
               climate_scenario=config["climate_scenarios"],
               )

rule surge_per_countries:
    input:
        "surge/surge_all_basins_{ssp}_{slr_year}slr_{climate_scenario}.hdf5",
    output:
        "surge/surge_{country}_{ssp}_{slr_year}slr_{climate_scenario}.hdf5",
    conda:
        "climada_env_TC"
    resources:
        mem_mb_per_cpu=24000,
    log:
        "logs/get_{country}_surge_{ssp}_{slr_year}slr_{climate_scenario}.log",
    script:
        "scripts/get_country_surge.py"
